# Weights Bias Activation

- W - Weights - multiplied to the inputs Wi
- b - Bias -  added at the end to the summation
- K - Activation - It is a function of each hidden neuron to bring the summation of weights and inputs multiplication to 0 to 1

![[Pasted image 20230526133507.png]]

# Math behind

- Cost function - Goodness of a model
- Derivatives
- Partial Derivatives
- Chain rule
	- Partial derivative of an indirectly related  value 


# Gradient Descent
-  An algorithm to minimize a function  by optimizing parameters
-  